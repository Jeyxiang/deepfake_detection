{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepfake detection using Deep learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. installing and importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install tensorflow\n",
    "# %pip install matplotlib\n",
    "# %pip install scipy\n",
    "# %pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\School work\\University Y3\\BT4240_CS4487\\PROJECT\\.venv\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "# setup output image format (Chrome works best)\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import zipfile\n",
    "import fnmatch\n",
    "import os.path\n",
    "import random\n",
    "import shutil\n",
    "import scipy\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D, Dropout\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.applications.resnet_v2 import ResNet50V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the version and python bits is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Python Version- 3.7.8\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow might not be able to be installed under different python versions\n",
    "from platform import python_version\n",
    "print(\"Current Python Version-\", python_version())\n",
    "\n",
    "\n",
    "# python 64 bits required\n",
    "import struct\n",
    "print(struct.calcsize(\"P\") * 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating file directories, extract image into respective directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "# ├── Test  \n",
    "# │   ├── manipulated\n",
    "# │   └── original\n",
    "# ├── Training\n",
    "# │   ├── manipulated \n",
    "# │   └── original\n",
    "# └── Validation\n",
    "#     ├── manipulated \n",
    "#     └── original\n",
    "\n",
    "# Creates the appropriate directory structures for training, validation and test sets.\n",
    "try:\n",
    "  shutil.rmtree('./Split Data')      \n",
    "except:\n",
    "  pass                #Split Data didn't exist\n",
    "     \n",
    "os.mkdir('./Split Data')\n",
    "cdf={\"Training\":0.7,\"Validation\":0.85,\"Test\":1} #OBS! Has to be increment percentages of 5 to make batch size fit\n",
    "for dir in list(cdf.keys()):\n",
    "    os.mkdir('./Split Data/{}'.format(dir))\n",
    "    os.mkdir('./Split Data/{}/manipulated'.format(dir))\n",
    "    os.mkdir('./Split Data/{}/original'.format(dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which assigns the file to the correct directory based on the discrete cumulative distribution function cdf\n",
    "\n",
    "def assign_data(cdf):\n",
    "    nbr=random.random()\n",
    "    for set in list(cdf.keys()):\n",
    "        if nbr<cdf[set]:\n",
    "            return set\n",
    "\n",
    "dist={\"Training\":0,\"Validation\":0,\"Test\":0}\n",
    "filename = 'data.zip'\n",
    "zfile = zipfile.ZipFile(filename, 'r')\n",
    "counter=0\n",
    "samplesize=12000\n",
    "# Each file is loaded in sequence and randomly assigned to the corresponding directory \n",
    "# in the new straucture according to cdf. Dictionary dist keeps track of number of each set.\n",
    "for name in zfile.namelist():\n",
    "    save_path = './Split Data/'\n",
    "    name_of_file=\"\"\n",
    "    label=\"\"\n",
    "    if fnmatch.fnmatch(name, \"data/manipulated/*.png\"):\n",
    "        name_of_file=name[len(\"data/manipulated/\"):]\n",
    "        label=\"manipulated\"\n",
    "    elif fnmatch.fnmatch(name,\"data/original/*.png\"):\n",
    "        name_of_file=name[len(\"data/original/\"):]\n",
    "        label=\"original\"\n",
    "    if name_of_file != \"\":\n",
    "        myfile = zfile.open(name)\n",
    "        img = matplotlib.image.imread(myfile)\n",
    "        rand_assign=assign_data(cdf)\n",
    "        dist[rand_assign]+=1\n",
    "        save_path+=rand_assign+\"/\"+label # eg. \"Split Data/Training/manipulated\n",
    "        completeName = os.path.join(save_path, name_of_file)         \n",
    "        matplotlib.image.imsave(completeName,img)\n",
    "        counter+=1\n",
    "        if counter>=samplesize:     \n",
    "            break\n",
    "\n",
    "zfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for checking the set sizes\n",
    "for fr in list(dist.keys()):\n",
    "    for label in [\"manipulated\",\"original\"]:\n",
    "        print(\"Size of {}/{}: {}\".format(fr,label,len(os.listdir(os.path.join('./Split Data',fr,label)))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building the model by initialising the attributes, layers etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A batch size of 32 means that 32 samples from the training dataset will be used to estimate the error gradient before the model weights are updated. \n",
    "- One training epoch means that the learning algorithm has made one pass through the training dataset, where examples were separated into randomly \n",
    "selected “batch size” groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant fields\n",
    "BATCH_SIZE= 50 \n",
    "imgsize=(299,299)\n",
    "COUNT_DENSE = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building ResNet Models\n",
    "model=Sequential()\n",
    "model.add(ResNet50V2(weights=\"imagenet\", include_top=False, pooling='avg'))   #include_top enables transfer learning. pooling arbitrary now.\n",
    "\n",
    "model.add(Dense(69,kernel_regularizer=regularizers.l2(0.001), activation=\"relu\")) # activation function\n",
    "model.add(Dropout(0.5)) # reduce overfitting - “0.5” specifies the amount of input to be removed from the available input data\n",
    "model.add(Dense(2,activation='softmax')) \n",
    "model.layers[0].trainable = False     #makes the ResNet50 untrainable, so only the Dense network is trained.\n",
    "\n",
    "\n",
    "sgd = optimizers.SGD(learning_rate = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True) #SGD arbitrary, there are others (Adam, etc)\n",
    "model.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics = ['accuracy'])   #Loss function arbitrary now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 13)                26637     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 13)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,591,465\n",
      "Trainable params: 26,665\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\tbaseModel = ResNet50V2(weights=\"imagenet\", include_top=False, pooling = 'avg',\n",
    "\tinput_tensor=Input(shape=(299, 299, 3)))\n",
    "\n",
    "\t# pooling - process of splitting the data into several regions, take max/avg of cells in those region, insert into an output matrix\n",
    "\t# pooling = avg: global average pooling applied to the output of the last convolutional block\n",
    "\n",
    "\t# construct the head of the model that will be placed on top of the\n",
    "\t# the base model, replace with own layers\n",
    "\n",
    "\t# TODO - determine the headModel layers! (What layers we choose to add)\n",
    "\theadModel = baseModel.output\n",
    "\t# headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "\t# pool size - size of the pooling window\n",
    "\n",
    "\t# headModel = Flatten(name=\"flatten\")(headModel) # Flattening the input - \n",
    "\n",
    "\tfor i in range(COUNT_DENSE):\n",
    "\t\theadModel = Dense(23, activation=\"relu\")(headModel) # activation function\n",
    " \n",
    "\n",
    "\theadModel = Dropout(0.5)(headModel) # reduce overfitting - “0.5” specifies the amount of input to be removed from the available input data\n",
    "\n",
    "\theadModel = Dense(2, activation=\"relu\")(headModel) # 2 = number of classes\n",
    "\n",
    "\t# place the head FC model on top of the base model (this will become\n",
    "\t# the actual model we will train)\n",
    "\tmodel = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "\t# loop over all layers in the base model and freeze them so they will\n",
    "\t# *not* be updated during the training process, keeping the initial layers intact\n",
    "\t# base model is the pre-trained model (imagenet), which will not be tampered with\n",
    "\n",
    "\t# Use SGD Optimizer\n",
    "\toptimizer = optimizers.SGD(learning_rate = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True) \n",
    "\t# for layer in baseModel.layers:\n",
    "\t# \tlayer.trainable = False\n",
    "\tmodel.compile(loss=\"binary_crossentropy\",optimizer=optimizer,metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8403 images belonging to 2 classes.\n",
      "Found 1812 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet_v2 import preprocess_input\n",
    "\n",
    "trainpath=os.path.join(\"./Split Data\", \"Training\")\n",
    "valpath=os.path.join(\"./Split Data\", \"Validation\")\n",
    "\n",
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_gen=train_datagen.flow_from_directory(trainpath, target_size=imgsize, batch_size=BATCH_SIZE, class_mode=\"categorical\")\n",
    "val_gen=train_datagen.flow_from_directory(valpath, target_size=imgsize, batch_size=BATCH_SIZE, class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialising callbacks, stoppers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- monitor – This allows us to specify the performance measure to monitor in order to end training.\n",
    "\n",
    "- mode – It is used to specify whether the objective of the chosen metric is to increase maximize or to minimize.\n",
    "\n",
    "- verbose – To discover the training epoch on which training was stopped, the “verbose” argument can be set to 1. Once stopped, the callback will print the epoch number.\n",
    "\n",
    "- patience – The first sign of no further improvement may not be the best time to stop training. This is because the model may coast into a plateau of no improvement or even get slightly worse before getting much better. We can account for this by adding a delay to the trigger in terms of the number of epochs on which we would like to see no improvement. This can be done by setting the “patience” argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  os.mkdir(os.path.join('.','Working Model'))       \n",
    "except:\n",
    "  pass                #Working Model aleardy exists\n",
    "cb_early_stopper = EarlyStopping(monitor = 'val_loss', patience = 4)\n",
    "\n",
    "cb_checkpointer = ModelCheckpoint(filepath =os.path.join(\".\",\"Working Model\",\"best.hdf5\") , monitor = 'val_loss', save_best_only = True, mode = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 169 50 37\n"
     ]
    }
   ],
   "source": [
    "#Just for double checking\n",
    "print(BATCH_SIZE, len(train_gen), BATCH_SIZE, len(val_gen))\n",
    "# os.mkdir(os.path.join('.','Working Model'))         used to create the Working Model directory\n",
    "EPOCHS=20\n",
    "# TRAIN_STEPS=int(len(train_gen)/EPOCHS)\n",
    "# VAL_STEPS=len(val_gen)\n",
    "# print(\"EPOCHS: {}, TRAINING STEPS: {}, VAL STEPS: {}\".format(EPOCHS, TRAIN_STEPS, VAL_STEPS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fitting, training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model()\n",
    "fit_history = model.fit_generator(\n",
    "        train_gen,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data=val_gen,\n",
    "        callbacks=[cb_checkpointer, cb_early_stopper]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training & validation error and loss function over epochs\n",
    "plt.figure(1, figsize = (15,8)) \n",
    "    \n",
    "plt.subplot(221)  \n",
    "plt.plot(fit_history.history['accuracy'])  \n",
    "plt.plot(fit_history.history['val_accuracy'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'valid']) \n",
    "    \n",
    "plt.subplot(222)  \n",
    "plt.plot(fit_history.history['loss'])  \n",
    "plt.plot(fit_history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'valid']) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0a1 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40a03b7b172bd2197b5b405d58bfc55505d4cc82ebca9376d27bab8203000ac3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
